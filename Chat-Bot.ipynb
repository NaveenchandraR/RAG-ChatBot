{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc21a27a-3bfb-4b3f-8dae-9578d4eac433",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -Uqqq transformers einops accelerate langchain langchain-community bitsandbytes tiktoken sentence-transformers\n",
    "!pip3 install -Uqqq langchain-chroma langchain_google_genai python-dotenv faiss-cpu langchain_huggingface streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d43283f9-2069-4868-8896-7b1210df80e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please Create a Gemini API Key using the following link\n",
    "# https://ai.google.dev/gemini-api/docs/api-key\n",
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24578917-7883-4fa5-8b70-34d0df03a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "def get_llm():\n",
    "\n",
    "    print(f\"Loading Environment Variables\")\n",
    "    load_dotenv()\n",
    "\n",
    "    print(f\"Loading LLM Module from Langchain\")\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key = os.environ[\"GOOGLE_API_KEY\"],temperature = 0.1)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31ab9f4f-ef7a-447b-aab6-00d20818fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "abspath = os.path.abspath('run_chat.py')\n",
    "dname = os.path.dirname(abspath)\n",
    "\n",
    "\n",
    "def get_embedding_method():\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embedding_function = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    return embedding_function\n",
    "\n",
    "\n",
    "def get_docs():\n",
    "    # Load documents locally as CSV\n",
    "    loader = CSVLoader(f'{dname}/data/Customer-Support.csv')\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Split document into text chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    docs = text_splitter.split_documents(docs)\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "def get_embeddings():\n",
    "    print(\"Loading Document\")\n",
    "    docs = get_docs()\n",
    "\n",
    "    print(\"Loading Embedding Function\")\n",
    "    embedding_function = get_embedding_method()\n",
    "    \n",
    "    print(\"Creating Vector DB\")\n",
    "    db = Chroma.from_documents(docs, embedding_function)\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b4ec89f-1272-438d-8b28-98ad476b75c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def get_prompt():\n",
    "    \n",
    "    prompt_templet = \"\"\"\n",
    "    You are a customer service chatbot\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Answer the customer's questions only using the source data provided. \n",
    "    If you are unsure, say \"I don't know, please call our customer support\". \n",
    "    Use engaging, courteous, and professional language similar to a customer representative.\n",
    "    Keep your answers concise.\n",
    "\n",
    "    Generate the answer only for the given question. Do not generate extra text\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "    template = prompt_templet,\n",
    "    input_variables = [\"context\",\"question\"]\n",
    "    )\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da84fbbd-9eaf-49e8-9f0b-c2ba1eb965d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "import streamlit as st\n",
    "\n",
    "import langchain\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from src.langchain_llm import get_llm\n",
    "from src.vectorDB_retriever import get_embeddings\n",
    "from src.prompt_template import get_prompt\n",
    "\n",
    "\n",
    "file = \"vector_database_file\"\n",
    "\n",
    "def get_chain():\n",
    "    \n",
    "    llm = get_llm()\n",
    "    embeddings = get_embeddings()\n",
    "    retriever = embeddings.as_retriever(search_kwargs={\"k\": 1})\n",
    "    prompt = get_prompt()\n",
    "    \n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        \n",
    "        llm = llm,\n",
    "        chain_type = \"stuff\",\n",
    "        retriever = retriever,\n",
    "        input_key = \"query\",\n",
    "        return_source_documents = True,\n",
    "        chain_type_kwargs = {\"prompt\":prompt}\n",
    "    \n",
    "    )\n",
    "    \n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2091dcf8-c99f-44a1-9688-7e7e8d2867a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Environment Variables\n",
      "Loading LLM Module from Langchain\n",
      "Loading Document\n",
      "Loading Embedding Function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/berserk/anaconda3/envs/test1/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Vector DB\n"
     ]
    }
   ],
   "source": [
    "chain = get_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "faa3b088-1166-42f5-b11d-7e75c3c0eca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I received a damaged product.\n",
      "We apologize for the inconvenience. Can you please provide a photo of the damaged product so we can assist you further?\n"
     ]
    }
   ],
   "source": [
    "question = \"I received a damaged product.\"\n",
    "print(question)\n",
    "response = chain.invoke(question)\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb581bcc-13cf-4d56-bfc7-58673ea6119d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can I get a refund for my purchase?\n",
      "We understand your concern. Please provide your order number and the reason for your refund request so we can assist you further.\n"
     ]
    }
   ],
   "source": [
    "question = \"Can I get a refund for my purchase?\"\n",
    "print(question)\n",
    "response = chain.invoke(question)\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "224251bf-a358-4dea-82f5-5f1891827f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can I add an item to an existing order?\n",
      "We'll do our best to help. Can you please provide your order number and the details of the item you'd like to add?\n"
     ]
    }
   ],
   "source": [
    "question = \"Can I add an item to an existing order?\"\n",
    "print(question)\n",
    "response = chain.invoke(question)\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc76c3-9a71-4a26-a7b2-e564755a8bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "test1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
